{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrument Classification - Part 1 : Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our first trial by trying to recognise piano (pia), acoustic guitar (gac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import math\n",
    "from random import shuffle\n",
    "from collections import deque\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirpia = './Instrument-Classification/IRMAS-TrainingData/pia/'\n",
    "dirgac = './Instrument-Classification/IRMAS-TrainingData/gac/'\n",
    "#dirdru = './Instrument-Classification/IRMAS-TrainingData/dru/'\n",
    "dirvoi = './Instrument-Classification/IRMAS-TrainingData/voi/'\n",
    "dirgac_voi = './Instrument-Classification/IRMAS-TrainingData/gac-voi/'\n",
    "\n",
    "#instr = ['pia','gac','dru','voi']\n",
    "#dir = [dirpia,dirgac,dirdru,dirvoi]\n",
    "\n",
    "instr = ['gac','voi']\n",
    "dir = [dirgac_voi,dirgac,dirvoi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(dir_array):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    list = []\n",
    "    for dir in dir_array:\n",
    "        for f in os.listdir(dir):\n",
    "            list.append(dir+f)\n",
    "    check = 0\n",
    "    for f in list:\n",
    "        label = f.split('/')\n",
    "        # downsampled to 22050Hz and downmixed to mono\n",
    "        wav,sr = librosa.load(f,mono=True,sr=22050)\n",
    "        # normalized by the root mean square energy\n",
    "        wav = wav / np.sqrt(np.mean(wav**2))\n",
    "        # chunked to 1sec long snippets\n",
    "        chunks = make_chunks(wav,sr)\n",
    "        for wav in chunks:\n",
    "            # transformed into mel-spectrograms with given traits\n",
    "            mel = librosa.feature.melspectrogram(wav,n_mels=96,fmax=11025,n_fft=1024,hop_length=256,power=1)\n",
    "            # decibel scaling\n",
    "            mel = librosa.core.amplitude_to_db(mel)\n",
    "            # saved to train and test arrays\n",
    "                \n",
    "            #just for checking\n",
    "            if (check%500==0): \n",
    "                print(check,\" chuncks have been made!\")\n",
    "            check+=1\n",
    "            y_list.append(label[-2])\n",
    "            x_list.append(mel)\n",
    "    return (x_list,y_list)\n",
    "\n",
    "def to_categorical(list):\n",
    "    temp_list = [[0]*len(instr)]*(len(list))\n",
    "    for i in range(len(list)):\n",
    "        x = [0]*len(instr)\n",
    "        x.insert(0,1)\n",
    "        x.pop()\n",
    "        for ins in instr:\n",
    "            if (ins in list[i]): \n",
    "                temp_list[i] = [sum(x) for x in zip(temp_list[i],x)]\n",
    "            x.insert(0,x.pop())\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  chuncks have been made!\n",
      "500  chuncks have been made!\n",
      "1000  chuncks have been made!\n",
      "1500  chuncks have been made!\n",
      "2000  chuncks have been made!\n",
      "2500  chuncks have been made!\n",
      "3000  chuncks have been made!\n",
      "3500  chuncks have been made!\n",
      "4000  chuncks have been made!\n",
      "4500  chuncks have been made!\n",
      "5000  chuncks have been made!\n",
      "5500  chuncks have been made!\n",
      "6000  chuncks have been made!\n",
      "6500  chuncks have been made!\n",
      "7000  chuncks have been made!\n",
      "7500  chuncks have been made!\n",
      "8000  chuncks have been made!\n",
      "8500  chuncks have been made!\n",
      "9000  chuncks have been made!\n",
      "9500  chuncks have been made!\n",
      "10000  chuncks have been made!\n",
      "10500  chuncks have been made!\n",
      "11000  chuncks have been made!\n",
      "11500  chuncks have been made!\n",
      "12000  chuncks have been made!\n",
      "12500  chuncks have been made!\n",
      "13000  chuncks have been made!\n",
      "13500  chuncks have been made!\n",
      "14000  chuncks have been made!\n",
      "14500  chuncks have been made!\n",
      "15000  chuncks have been made!\n",
      "15500  chuncks have been made!\n",
      "16000  chuncks have been made!\n",
      "16500  chuncks have been made!\n",
      "17000  chuncks have been made!\n",
      "17500  chuncks have been made!\n",
      "18000  chuncks have been made!\n",
      "18500  chuncks have been made!\n",
      "19000  chuncks have been made!\n",
      "19500  chuncks have been made!\n",
      "Train Size: (17980, 96, 87, 1)\n",
      "Test  Size: (1998, 96, 87, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_list,y_list) = pre_processing(dir)\n",
    "\n",
    "y_final = []\n",
    "for label in y_list:\n",
    "    y = label.split('-')\n",
    "    y_final.append(y)\n",
    "\n",
    "x_list = np.expand_dims(np.array(x_list),-1)\n",
    "y_list = np.array(to_categorical(y_final))\n",
    "\n",
    "#shuffling sets\n",
    "id = np.random.permutation(len(x_list))\n",
    "x_list,y_list = x_list[id], y_list[id]\n",
    "\n",
    "#splitting to train and test 90/10\n",
    "split_index = math.floor(len(x_list)*0.90)\n",
    "x_train = x_list[:split_index]\n",
    "y_train = y_list[:split_index]\n",
    "x_test  = x_list[split_index:]\n",
    "y_test  = y_list[split_index:]\n",
    "\n",
    "print(\"Train Size:\", x_train.shape)\n",
    "print(\"Test  Size:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_trainGVm.npy',x_train)\n",
    "np.save('x_testGVm.npy' ,x_test)\n",
    "np.save('y_trainGVm.npy',y_train)\n",
    "np.save('y_testGVm.npy' ,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
