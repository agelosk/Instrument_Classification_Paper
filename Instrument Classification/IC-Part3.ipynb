{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrument Classification - Part 3 : Track Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import keras\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_song(dir,songs):\n",
    "    predict = []\n",
    "    even = True\n",
    "    it = 0\n",
    "    for f in os.listdir(dir):\n",
    "        if (it == songs): break\n",
    "        if (even): \n",
    "            even = False \n",
    "            continue\n",
    "        else:\n",
    "            even = True\n",
    "            # downsampled to 22050Hz and downmixed to mono\n",
    "            wav,sr = librosa.load(dir+f,mono=True,sr=22050)\n",
    "            # normalized by the root mean square energy\n",
    "            wav = wav / np.sqrt(np.mean(wav**2))\n",
    "            # chunked to 1sec long snippets\n",
    "            chunks = make_chunks(wav,sr)\n",
    "            for wav in chunks:\n",
    "                # transformed into mel-spectrograms with given traits\n",
    "                mel = librosa.feature.melspectrogram(wav,n_mels=96,fmax=11025,n_fft=1024,hop_length=256,power=1)\n",
    "                # decibel scaling\n",
    "                mel = librosa.core.amplitude_to_db(mel)\n",
    "                # zero padding when mel shape is not (96,87)\n",
    "                pad = np.zeros((96,87))\n",
    "                if (mel.shape != pad.shape):\n",
    "                    pad[:mel.shape[0],:mel.shape[1]] = mel\n",
    "                    mel = pad\n",
    "                predict.append(mel)\n",
    "        it = it + 1\n",
    "    return predict\n",
    "\n",
    "def extract_instruments_from_prediction(prediction):\n",
    "    # connect to_categorical to instrument labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = './IRMAS/TestingData/Part1/'\n",
    "predict = crop_song(dir,1)\n",
    "predict_new = np.expand_dims(np.array(predict),-1)\n",
    "model = keras.models.load_model('ICmodel_new.h5')\n",
    "prediction = model.predict(predict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "instr = []\n",
    "for i in range(0,len(predict)):\n",
    "    #instr.append([])\n",
    "    print(np.argmax(prediction[i]))\n",
    "    #instr[i].append(extract_instruments_from_prediction(prediction))\n",
    "\n",
    "# printed example:\n",
    "# 0-1s : guitar,piano\n",
    "# 1-2s : guitar,piano\n",
    "# 2-3s : guitar\n",
    "# 3-4s : guitar,voice\n",
    "# 4-5s : voice\n",
    "# 5-end: piano"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
